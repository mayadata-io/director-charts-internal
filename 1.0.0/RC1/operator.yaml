#TODO this will be removed when we use dockerhub public repos
apiVersion: v1
kind: Secret
data:
  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOiB7CgkJCSJhdXRoIjogImJXRjVZV1JoZEdFNlRXRjVZVVJ2WTJ0bGNrQXhNak09IgoJCX0KCX0sCgkiSHR0cEhlYWRlcnMiOiB7CgkJIlVzZXItQWdlbnQiOiAiRG9ja2VyLUNsaWVudC8xNy4xMi4xLWNlIChsaW51eCkiCgl9Cn0=
metadata:
  name: mayadatasecret
type: kubernetes.io/dockerconfigjson
---
#nginx deployment
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    app: default-http-backend
  namespace: ingress-nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: default-http-backend
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: default-http-backend
        # Any image is permissable as long as:
        # 1. It serves a 404 page at /
        # 2. It serves 200 on a /healthz endpoint
        image: gcr.io/google_containers/defaultbackend:1.4
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
          requests:
            cpu: 10m
            memory: 20Mi
---

apiVersion: v1
kind: Service
metadata:
  name: default-http-backend
  namespace: ingress-nginx
  labels:
    app: default-http-backend
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: default-http-backend
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      hostNetwork: true
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.17.0
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
---
#nginx service for node port
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
#Storage components
#cassandra
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.variablename.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  # These are converted to volume claims by the controller
  # and mounted at the paths mentioned above.
  # do not use these in production until ssd GCEPersistentDisk or other ssd pd
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: cstor-storage-class
      resources:
        requests:
          storage: 40Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  clusterIP: None
  ports:
  - port: 9042
  selector:
    app: cassandra
---
#mysql deployment
#mysql configmap
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: mysql
  name: mysql
  namespace: variablename
data:
  master.cnf: |
    # Apply this config only on the master.
    [mysqld]
    log-bin
    expire_logs_days=3
    max_allowed_packet=16M
  slave.cnf: |
    # Apply this config only on slaves.
    [mysqld]
    super-read-only
---
#mysql service
apiVersion: v1
kind: Service
metadata:
  labels:
    app: mysql
  name: mysql
  namespace: variablename
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: mysql
---
#mysql sts
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  namespace: variablename
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        - name: MYSQL_DATABASE
          value: "maya"
        - name: MYSQL_USER
          value: "maya"
        - name: MYSQL_PASSWORD
          value: "maya"
        image: mysql:5.7
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
          failureThreshold: 3
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: mysql
        ports:
        - containerPort: 3306
          name: mysql
          protocol: TCP
        readinessProbe:
          exec:
            command:
            - mysql
            - -h
            - 127.0.0.1
            - -e
            - SELECT 1
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/mysql
          name: data
          subPath: mysql
        - mountPath: /etc/mysql/conf.d
          name: conf
      - command:
        - bash
        - -c
        - |
          set -ex
          cd /var/lib/mysql

          # Determine binlog position of cloned data, if any.
          if [[ -f xtrabackup_slave_info ]]; then
            # XtraBackup already generated a partial "CHANGE MASTER TO" query
            # because we're cloning from an existing slave.
            mv xtrabackup_slave_info change_master_to.sql.in
            # Ignore xtrabackup_binlog_info in this case (it's useless).
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # We're cloning directly from master. Parse binlog position.
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi

          # Check if we need to complete a clone by starting replication.
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done

            echo "Initializing replication from clone position"
            # In case of container restart, attempt this at-most-once.
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi

          # Start a server to send backups when requested by peers.
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        image: gcr.io/google-samples/xtrabackup:1.0
        imagePullPolicy: IfNotPresent
        name: xtrabackup
        ports:
        - containerPort: 3307
          name: xtrabackup
          protocol: TCP
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/mysql
          name: data
          subPath: mysql
        - mountPath: /etc/mysql/conf.d
          name: conf
      dnsPolicy: ClusterFirst
      initContainers:
      - command:
        - bash
        - -c
        - |
          set -ex
          # Generate mysql server-id from pod ordinal index.
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # Add an offset to avoid reserved server-id=0 value.
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # Copy appropriate conf.d files from config-map to emptyDir.
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        image: mysql:5.7
        imagePullPolicy: IfNotPresent
        name: init-mysql
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /mnt/conf.d
          name: conf
        - mountPath: /mnt/config-map
          name: config-map
      - command:
        - bash
        - -c
        - |
          set -ex
          # Skip the clone if data already exists.
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Skip the clone on master (ordinal index 0).
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # Clone data from previous peer.
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # Prepare the backup.
          xtrabackup --prepare --target-dir=/var/lib/mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        imagePullPolicy: IfNotPresent
        name: clone-mysql
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/mysql
          name: data
          subPath: mysql
        - mountPath: /etc/mysql/conf.d
          name: conf
      restartPolicy: Always
      schedulerName: variablename-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: conf
      - hostPath:
          type: DirectoryOrCreate
          path: /mysqldata
        name: data
      - configMap:
          variablenameMode: 420
          name: mysql
        name: config-map
#  volumeClaimTemplates:
#  - metadata:
#      creationTimestamp: null
#      name: data
#    spec:
#      accessModes:
#      - ReadWriteOnce
#      resources:
#        requests:
#          storage: 100Gi
#      storageClassName: cstor-storage-class
---
#elasticsearch service
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: elasticsearch-logging
  name: elasticsearch-logging
  namespace: variablename
spec:
  ports:
  - port: 9200
    protocol: TCP
    targetPort: db
  selector:
    k8s-app: elasticsearch-logging
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: variablename
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
subjects:
- kind: ServiceAccount
  name: elasticsearch-logging
  namespace: variablename
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: elasticsearch-logging
  apiGroup: ""
--- 
# RBAC authn and authz
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch-logging
  namespace: variablename
  labels:
    k8s-app: elasticsearch-logging
   
---
#elaticsearch sts
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    k8s-app: elasticsearch-logging
  name: elasticsearch-logging
  namespace: variablename
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: elasticsearch-logging
  serviceName: elasticsearch-logging
  template:
    metadata:
      labels:
        k8s-app: elasticsearch-logging
    spec:
      containers:
      - env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: NODE_MASTER
          value: "false"
        - name: ES_JAVA_OPTS
          value: -Xms512m -Xmx512m
        image: mayadataio/elasticsearch-search-guard:88a263ec
        imagePullPolicy: IfNotPresent
        name: elasticsearch-logging
        ports:
        - containerPort: 9200
          name: db
          protocol: TCP
        - containerPort: 9300
          name: transport
          protocol: TCP
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        securityContext:
          runAsUser: 1000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: elasticsearch-logging
      imagePullSecrets:
      - name: mayadatasecret
      initContainers:
      - command:
        - /sbin/sysctl
        - -w
        - vm.max_map_count=262144
        image: alpine:3.6
        imagePullPolicy: IfNotPresent
        name: elasticsearch-logging-init
        resources: {}
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      restartPolicy: Always
      schedulerName: variablename-scheduler
      securityContext:
        fsGroup: 1000
      serviceAccount: elasticsearch-logging
      serviceAccountName: elasticsearch-logging
  volumeClaimTemplates:
  - metadata:
      annotations:
      name: elasticsearch-logging
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 200Gi
      storageClassName: cstor-storage-class    
---
#cortex deployment
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: alertmanager
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: alertmanager
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: am-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding";
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}');
             echo $MAYA;
          done
      - name: am-wait-for-configs
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for configs"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=configs -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w configs | wc -l)
             echo $COUNT
          done
      - name: am-wait-for-maya-io
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for maya-io-server"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=maya-io -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w maya-io | wc -l)
             echo $COUNT
          done
      containers:
      - name: alertmanager
        image: openebs/cortex-alertmanager:alerts-0038d54e
        imagePullPolicy: Always
        args:
        - -log.level=debug
        - -server.http-listen-port=80
        - -alertmanager.configs.url=http://configs.variablename.svc.cluster.local:80
        - -alertmanager.web.external-url=/api/prom/alertmanager
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
spec:
  ports:
    - port: 80
  selector:
    name: alertmanager
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: configs-db
  namespace: variablename
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: configs-db
      annotations:
        prometheus.io.scrape: "false"
    spec:
      containers:
      - name: configs-db
        image: postgres:9.6
        imagePullPolicy: IfNotPresent
        env:
          - name: POSTGRES_DB
            value: configs
        ports:
        - containerPort: 5432
---
apiVersion: v1
kind: Service
metadata:
  name: configs-db
  namespace: variablename
spec:
  ports:
    - port: 5432
  selector:
    name: configs-db
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: configs
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: configs
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: cf-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      initContainers:
      - name: cf-wait-for-configs-db
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for configs-db"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=configs-db -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w configs-db | wc -l)
             echo $COUNT
          done
      containers:
      - name: configs
        image: openebs/cortex-configs:alerts-b454a216
        imagePullPolicy: Always
        args:
        - -server.http-listen-port=80
        - -database.uri=postgres://postgres@configs-db.variablename.svc.cluster.local/configs?sslmode=disable
        - -database.migrations=/migrations
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: configs
spec:
  ports:
    - port: 80
  selector:
    name: configs
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: consul
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: consul
    spec:
      containers:
      - name: consul
        image: consul:0.7.1
        imagePullPolicy: IfNotPresent
        args:
        - agent
        - -ui
        - -server
        - -client=0.0.0.0
        - -bootstrap
        env:
        - name: CHECKPOINT_DISABLE
          value: "1"
        ports:
        - name: server-noscrape
          containerPort: 8300
        - name: serf-noscrape
          containerPort: 8301
        - name: client-noscrape
          containerPort: 8400
        - name: http-noscrape
          containerPort: 8500
---
apiVersion: v1
kind: Service
metadata:
  name: consul
spec:
  ports:
  - name: http
    port: 8500
  selector:
    name: consul
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: distributor
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: distributor
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: di-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: di-wait-for-consul
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for consul"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=consul -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w consul | wc -l)
             echo $COUNT
          done
      - name: di-wait-for-ingester
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for ingester"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=ingester -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w ingester | wc -l)
             echo $COUNT
          done
      containers:
      - name: distributor
        image: quay.io/cortexproject/distributor:master-590e72c6
        imagePullPolicy: IfNotPresent
        args:
        - -log.level=debug
        - -server.http-listen-port=80
        - -consul.hostname=consul.variablename.svc.cluster.local:8500
        - -distributor.replication-factor=1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: distributor
spec:
  ports:
    - port: 80
  selector:
    name: distributor
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ingester
spec:
  replicas: 3

  # Ingesters are not ready for at least 1 min
  # after creation.  This has to be in sync with
  # the ring timeout value, as this will stop a
  # stampede of new ingesters if we should loose
  # some.
  minReadySeconds: 60

  # Having maxSurge 0 and maxUnavailable 1 means
  # the deployment will update one ingester at a time
  # as it will have to stop one (making one unavailable)
  # before it can start one (surge of zero)
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  template:
    metadata:
      labels:
        name: ingester
    spec:
      # Give ingesters 40 minutes grace to flush chunks and exit cleanly.
      # Service is available during this time, as long as we don't stop
      # too many ingesters at once.
      terminationGracePeriodSeconds: 300

      serviceAccountName: sa-maya-io
      initContainers:
      - name: in-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: in-wait-for-cassandra
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for cassandra"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=cassandra -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w cassandra | wc -l)
             echo $COUNT
          done
      - name: in-wait-for-consul
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for consul"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=consul -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w consul | wc -l)
             echo $COUNT
          done
      - name: in-wait-for-table-manager
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for table-manager"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=table-manager -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w table-manager | wc -l)
             echo $COUNT
          done
      containers:
      - name: ingester
        image: quay.io/cortexproject/ingester:master-590e72c6
        imagePullPolicy: IfNotPresent
        args:
        - -ingester.join-after=30s
        - -ingester.claim-on-rollout=true
        - -consul.hostname=consul.variablename.svc.cluster.local:8500
        - -dynamodb.original-table-name=cortex
        - -chunk.storage-client=cassandra
        - -cassandra.addresses=cassandra.variablename.svc.cluster.local
        - -cassandra.keyspace=mayacortex
        - -dynamodb.periodic-table.prefix=cortex_weekly_
        - -dynamodb.periodic-table.from=2017-01-06
        - -dynamodb.daily-buckets-from=2017-01-10
        - -dynamodb.base64-buckets-from=2017-01-17
        - -dynamodb.v4-schema-from=2017-02-05
        - -dynamodb.v5-schema-from=2017-02-22
        - -dynamodb.v6-schema-from=2017-03-19
        - -dynamodb.chunk-table.from=2017-04-17
        - -memcached.hostname=memcached.variablename.svc.cluster.local
        - -memcached.timeout=100ms
        - -memcached.service=memcached
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 15
          timeoutSeconds: 1
---
apiVersion: v1
kind: Service
metadata:
  name: ingester
spec:
  ports:
    - port: 80
  selector:
    name: ingester
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: memcached
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: memcached
      annotations:
        prometheus.io.scrape: "false"
    spec:
      containers:
      - name: memcached
        image: memcached:1.4.25
        imagePullPolicy: IfNotPresent
        args:
        - -m 64 # Maximum memory to use, in megabytes. 64MB is variablename.
        - -p 11211 # Default port, but being explicit is nice.
        ports:
        - name: clients
          containerPort: 11211
        securityContext:
          runAsUser: 1000
      securityContext:
        fsGroup: 1000
---
apiVersion: v1
kind: Service
metadata:
  name: memcached
spec:
  # The memcache client uses DNS to get a list of memcached servers and then
  # uses a consistent hash of the key to determine which server to pick.
  clusterIP: None
  ports:
    - name: memcached
      port: 11211
    - name: prom
      port: 9150
  selector:
    name: memcached
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: querier
spec:
  replicas: 3
  template:
    metadata:
      labels:
        name: querier
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: qu-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: qu-wait-for-cassandra
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for cassandra"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=cassandra -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w cassandra | wc -l)
             echo $COUNT
          done
      - name: qu-wait-for-consul
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for consul"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=consul -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w consul | wc -l)
             echo $COUNT
          done
      - name: qu-wait-for-table-manager
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for table-manager"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=table-manager -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w table-manager | wc -l)
             echo $COUNT
          done
      containers:
      - name: querier
        image: quay.io/cortexproject/querier:master-590e72c6
        imagePullPolicy: IfNotPresent
        args:
        - -server.http-listen-port=80
        - -consul.hostname=consul.variablename.svc.cluster.local:8500
        - -dynamodb.original-table-name=cortex
        - -chunk.storage-client=cassandra
        - -cassandra.addresses=cassandra.variablename.svc.cluster.local
        - -cassandra.keyspace=mayacortex
        - -dynamodb.periodic-table.prefix=cortex_weekly_
        - -dynamodb.periodic-table.from=2017-01-06
        - -dynamodb.daily-buckets-from=2017-01-10
        - -dynamodb.base64-buckets-from=2017-01-17
        - -dynamodb.v4-schema-from=2017-02-05
        - -dynamodb.v5-schema-from=2017-02-22
        - -dynamodb.v6-schema-from=2017-03-19
        - -dynamodb.chunk-table.from=2017-04-17
        - -memcached.hostname=memcached.variablename.svc.cluster.local
        - -memcached.timeout=100ms
        - -memcached.service=memcached
        - -distributor.replication-factor=1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: querier
spec:
  ports:
    - port: 80
  selector:
    name: querier
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ruler
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: ruler
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: rl-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: rl-wait-for-cassandra
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for cassandra"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=cassandra -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w cassandra | wc -l)
             echo $COUNT
          done
      - name: rl-wait-for-consul
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for consul"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=consul -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w consul | wc -l)
             echo $COUNT
          done
      - name: rl-wait-for-ingester
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for ingester"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=ingester -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w ingester | wc -l)
             echo $COUNT
          done
      - name: rl-wait-for-querier
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for querier"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=querier -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w querier | wc -l)
             echo $COUNT
          done
      - name: rl-wait-for-alertmanager
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for alertmanager"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=alertmanager -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w alertmanager | wc -l)
             echo $COUNT
          done
      containers:
      - name: ruler
        image: quay.io/cortexproject/ruler:master-590e72c6
        imagePullPolicy: IfNotPresent
        args:
        - -log.level=debug
        - -server.http-listen-port=80
        - -ruler.configs.url=http://configs.variablename.svc.cluster.local:80
        - -ruler.alertmanager-url=http://alertmanager.variablename.svc.cluster.local/api/prom/alertmanager/
        - -consul.hostname=consul.variablename.svc.cluster.local:8500
        - -dynamodb.original-table-name=cortex
        - -chunk.storage-client=cassandra
        - -cassandra.addresses=cassandra.variablename.svc.cluster.local
        - -cassandra.keyspace=mayacortex
        - -dynamodb.periodic-table.prefix=cortex_weekly_
        - -dynamodb.periodic-table.from=2017-01-06
        - -dynamodb.daily-buckets-from=2017-01-10
        - -dynamodb.base64-buckets-from=2017-01-17
        - -dynamodb.v4-schema-from=2017-02-05
        - -dynamodb.v5-schema-from=2017-02-22
        - -dynamodb.v6-schema-from=2017-03-19
        - -dynamodb.chunk-table.from=2017-04-17
        - -memcached.hostname=memcached.variablename.svc.cluster.local
        - -memcached.timeout=100ms
        - -memcached.service=memcached
        - -distributor.replication-factor=1
        - -ruler.num-workers=64
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: ruler
spec:
  ports:
    - port: 80
  selector:
    name: ruler
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: table-manager
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: table-manager
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: init-agent
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: tm-wait-for-cassandra
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for cassandra"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=cassandra -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w cassandra | wc -l)
             echo $COUNT
          done
      containers:
      - name: table-manager
        image: quay.io/cortexproject/table-manager:master-590e72c6
        imagePullPolicy: IfNotPresent
        args:
        - -server.http-listen-port=80
        - -dynamodb.original-table-name=cortex
        - -chunk.storage-client=cassandra
        - -cassandra.addresses=cassandra.variablename.svc.cluster.local
        - -cassandra.keyspace=mayacortex
        - -dynamodb.periodic-table.prefix=cortex_weekly_
        - -dynamodb.periodic-table.from=2017-01-06
        - -dynamodb.chunk-table.from=2017-04-17
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: table-manager
spec:
  ports:
    - port: 80
  selector:
    name: table-manager
---
#alertstore deployment
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: alertstore
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: alertstore
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: init-agent
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: as-wait-for-cassandra
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for cassandra"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=cassandra -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w cassandra | wc -l)
             echo $COUNT
          done
      - name: as-wait-for-as-tablemanager
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for alertstore-tablemanager"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=alertstore-tablemanager -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w alertstore-tablemanager | wc -l)
             echo $COUNT
          done
      containers:
      - name: alertstore
        image: index.docker.io/mayadata/alertstore:1.0.0-RC1.ee
        imagePullPolicy: Always
        args:
        - -server.http-listen-port=80
        - -storage.storage-client=cassandra
#        - -cassandra.addresses=172.17.0.4
        - -cassandra.addresses=cassandra.variablename.svc.cluster.local
        - -cassandra.keyspace=mayaalerts
#        - -storage.table-duration=5m
#        - -cassandra.table-duration=5m
        - -log.level=debug
        ports:
        - containerPort: 80 
---
apiVersion: v1
kind: Service
metadata:
  name: alertstore
spec:
  type: ClusterIP
  ports:
    - port: 80
  selector:
    name: alertstore
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: alertstore-tablemanager
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: alertstore-tablemanager
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: astm-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: astm-wait-for-cassandra
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for cassandra"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=cassandra -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w cassandra | wc -l)
             echo $COUNT
          done
      containers:
      - name: alertstore-tablemanager
        image: ndex.docker.io/mayadata/alertstore-tablemanager:1.0.0-RC1.ee
        imagePullPolicy: Always
        args:
        - -server.http-listen-port=80
        - -storage.storage-client=cassandra
#        - -cassandra.addresses=172.17.0.4
        - -cassandra.addresses=cassandra.variablename.svc.cluster.local
        - -cassandra.keyspace=mayaalerts
#        - -cassandra.table-duration=5m
#        - -tablemanager.table-duration=5m
        - -log.level=debug
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: alertstore-tablemanager
spec:
  type: ClusterIP
  ports:
    - port: 80
  selector:
    name: alertstore-tablemanager
---
#grafana deployment
#grafana configmap
# grafana-config
apiVersion: v1
kind: ConfigMap
metadata:
  name: maya-grafana-cfgmap
data:
  grafana.ini: |
    #################################### Server ####################################
    [server]
    # The public facing domain name used to access grafana from a browser
    domain = ${URL} 
    # If you use reverse proxy and sub path specify full url (with sub path)
    root_url = %(protocol)s://%(domain)s:/grafana
    [users]
    auto_assign_org = false
    # Default UI theme ("dark" or "light")
    variablename_theme = light
    [auth.proxy]
    enabled = true
    header_name = X-Scope-OrgID
    header_property = username
    auto_sign_up = true
    disable_signout_menu = true
    disable_login_form = true
---
#grafana pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-claim
spec:
  storageClassName: cstor-storage-class
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 40G
---
#grafana deployment
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: maya-grafana
  name: maya-grafana
spec:
  template:
    metadata:
      labels:
        app: maya-grafana
    spec:
      containers:
      - image: index.docker.io/mayadata/graph-reporter:1.0.0-RC1.ee
        name: grafana-reporter
        imagePullPolicy: Always
        ports:
        - containerPort: 8686
      - env:
        - name: GF_AUTH_BASIC_ENABLED
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "false"
        image: index.docker.io/mayadata/maya-grafana:1.0.0-RC1.ee
        imagePullPolicy: Always
        name: master-grafana
        ports:
        - containerPort: 3000
        volumeMounts:
        - mountPath: /etc/grafana
          name: config-volume
        - mountPath: /var/lib/grafana
          name: maya-grafana-store
      securityContext:
        fsGroup: 107
        runAsUser: 104
      imagePullSecrets:
      - name: mayadatasecret        
      volumes:
      - configMap:
          name: maya-grafana-cfgmap
        name: config-volume
      - name: maya-grafana-store
        persistentVolumeClaim:
          claimName: grafana-claim
---
#grafana svc
apiVersion: v1
kind: Service
metadata:
  annotations:
  name: maya-grafana-service
  namespace: variablename
spec:
  ports:
  - nodePort: 30378
    port: 3000
    protocol: TCP
    targetPort: 3000
    name: grafana
  - nodePort: 30379
    port: 8686
    protocol: TCP
    targetPort: 8686
    name: grafana-report
  selector:
    app: maya-grafana
  type: NodePort
---  
#kibana deployment yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    k8s-app: kibana-logging
  name: kibana-logging
  namespace: variablename
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kibana-logging
  template:
    metadata:
      creationTimestamp: null
      labels:
        k8s-app: kibana-logging
    spec:
      containers:
      - env:
        - name: SERVER_BASEPATH
          value: /kibana
        image: mayadataio/kibana:f960e137
        imagePullPolicy: Always
        name: kibana-logging
        ports:
        - containerPort: 5601
          name: ui
          protocol: TCP
        resources:
          limits:
            cpu: "1"
          requests:
            cpu: 100m
      imagePullSecrets:
      - name: mayadatasecret
---
#kibana svc
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: kibana-logging
  name: kibana-logging
  namespace: variablename
spec:
  ports:
  - port: 5601
    protocol: TCP
    targetPort: 5601
  selector:
    k8s-app: kibana-logging
  type: ClusterIP     
---
#ingress
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/auth-realm: Authentication Required
      nginx.ingress.kubernetes.io/auth-response-headers: username
      nginx.ingress.kubernetes.io/auth-secret: basic-auth
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/configuration-snippet: |
        proxy_set_header X-Scope-OrgID $remote_user;
      nginx.ingress.kubernetes.io/rewrite-target: /
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
    creationTimestamp: 2019-05-13T14:30:48Z
    generation: 5
    name: maya-auth-ingress
    namespace: variablename
    resourceVersion: "4416837"
    selfLink: /apis/extensions/v1beta1/namespaces/variablename/ingresses/maya-auth-ingress
    uid: b3d51ca4-758b-11e9-9b05-0cc47a805c04
  spec:
    rules:
    - http:
        paths:
        - backend:
            serviceName: distributor
            servicePort: 80
          path: /maya-cortex-push/
        - backend:
            serviceName: querier
            servicePort: 80
          path: /maya-cortex-pull/
        - backend:
            serviceName: alertmanager
            servicePort: 80
          path: /maya-cortex-am/
        - backend:
            serviceName: configs
            servicePort: 80
          path: /maya-cortex-configs/
        - backend:
            serviceName: alertstore
            servicePort: 80
          path: /alertstore/
        - backend:
            serviceName: alertstore-tablemanager
            servicePort: 80
          path: /alertstore-tablemanager/
        - backend:
            serviceName: retrieval
            servicePort: 80
          path: /retrieval/
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers X-Frame-Options SAMEORIGIN;
      nginx.ingress.kubernetes.io/rewrite-target: /
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
    name: maya-nginx-ingress
    namespace: variablename
  spec:
    rules:
    - http:
        paths:
        - backend:
            serviceName: maya-io
            servicePort: 8080
          path: /
        - backend:
            serviceName: echoheaders
            servicePort: 80
          path: /echo/
        - backend:
            serviceName: maya-ui
            servicePort: 80
          path: /ui/
        - backend:
            serviceName: chat-server
            servicePort: 8080
          path: /chat-server/
        - backend:
            serviceName: killbill-server
            servicePort: 8080
          path: /killbill-server/
        - backend:
            serviceName: maya-grafana-service
            servicePort: 8686
          path: /report/
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
---
#maya-ui
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
  name: maya-ui
  namespace: variablename
spec:
  selector:
    matchLabels:
      app: maya-ui
  template:
    metadata:
      labels:
        app: maya-ui
    spec:
      initContainers:
      - name: config-data
        image: mayadata/vendor:1.0.0-RC1.ee
        imagePullPolicy: Always
        securityContext:
          privileged: true
        env:
        - name: VENDOR
          value: "mod"
        command: ["sh","-c","./entrypoint.sh"]
        volumeMounts:
        - name: config-data
          mountPath: /tmp/mo-vendor
          mountPropagation: Bidirectional
      containers:
      - image: index.docker.io/mayadata/maya-ui:1.0.0-RC1.ee
        imagePullPolicy: Always
        name: maya-ui
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
        env:
        - name: VENDOR
          value: "mod"
        volumeMounts:
        - name: config-data
          mountPath: /tmp/mo-vendor
      volumes:
        - name: config-data
          hostPath:
            path: /tmp/mo-vendor
      imagePullSecrets:
      - name: mayadatasecret
---
#maya-ui svc
apiVersion: v1
kind: Service
metadata:
  labels:
    app: maya-ui
  name: maya-ui
  namespace: variablename
spec:
  ports:
  - name: maya-ui
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: maya-ui
---
#chat serever secret
apiVersion: v1
data:
  ADMIN_API_KEY: T0RsRk5qWkVRMFZGT0RkQ1JUTkdOekV3TVRJNmNUTjJkM2hsUW5Wa1NFaFJkbWd5WlhWYWNuaDFSRUp0TVVKeU9YYzJaa05HZFhsMWIyaFNNdz09
  MAYA_SERVER_IP: aHR0cHM6Ly9hcHAubWF5YW9ubGluZS5pby92My8=
  SERVER_ADDRESS: bWF5YW9ubGluZS5pbw==
  SERVER_NAME: TWF5YU9ubGluZQ==
  SIGNING_SECRET: OWNlZTAyNWI1ZTcxNmIxYjk4MDRiZGZmNGEyNzYyMzQ=
  SLASH_COMMAND: L21heWE=
  SUPPORT_EMAIL: c3VwcG9ydEBtYXlhZGF0YS5pbw==
kind: Secret
metadata:
  labels:
    app: chat-server
  name: chat-server
  namespace: variablename
type: Opaque
---
#maya-chat-server deployment
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: chat-server
  name: chat-server
  namespace: variablename
spec:
  selector:
    matchLabels:
      app: chat-server
  template:
    metadata:
      labels:
        app: chat-server
    spec:
      containers:
      - env:
        - name: ADMIN_API_KEY
          valueFrom:
            secretKeyRef:
              key: ADMIN_API_KEY
              name: chat-server
        - name: SIGNING_SECRET
          valueFrom:
            secretKeyRef:
              key: SIGNING_SECRET
              name: chat-server
        - name: MAYA_SERVER_IP
          valueFrom:
            secretKeyRef:
              key: MAYA_SERVER_IP
              name: chat-server
        - name: SLASH_COMMAND
          valueFrom:
            secretKeyRef:
              key: SLASH_COMMAND
              name: chat-server
        image: mayadata/maya-chatops:1.0.0-RC1.ee
        imagePullPolicy: Always
        name: chat-server
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
      imagePullSecrets:
      - name: mayadatasecret
---
#chat server svc 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: chat-server
  name: chat-server
  namespace: variablename
spec:
  ports:
  - name: maya-chatops
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: chat-server
---
#maya-io-pvc
#grafana pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mayastore
spec:
  storageClassName: cstor-storage-class
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 40G
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-maya-io
  namespace: default

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: crb-maya-io
  namespace: default
subjects:
- kind: ServiceAccount
  name: sa-maya-io
  namespace: default
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: maya-io
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app : maya-io
    spec:
      serviceAccountName: sa-maya-io
      initContainers:
      - name: mios-wait-for-crb
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
          echo $MAYA
          until [ ! -z "$MAYA" ]
          do
             echo "wating for ClusterRoleBinding"
             sleep 1;
             export MAYA=$(kubectl get ClusterRoleBinding | grep -w crb-maya-io | awk '{print $1}')
             echo $MAYA
          done
      - name: mios-wait-for-mysql
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for mysql"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=mysql -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w mysql | wc -l)
             echo $COUNT
          done
      - name: mios-wait-for-chat-server
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for chat-server"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=chat-server -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w chat-server | wc -l)
             echo $COUNT
          done
      - name: mios-wait-for-maya-grafana-service
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for maya-grafana"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=maya-grafana -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w maya-grafana | wc -l)
             echo $COUNT
          done
      - name: mios-wait-for-maya-ui
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for maya-ui"
             sleep 2;
             export COUNT=$(kubectl get pods -l app=maya-ui -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w maya-ui | wc -l)
             echo $COUNT
          done
      - name: mios-wait-for-elasticsearch-logging
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for elasticsearch-logging"
             sleep 2;
             export COUNT=$(kubectl get pods -l k8s-app=elasticsearch-logging -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w elasticsearch-logging | wc -l)
             echo $COUNT
          done
      - name: mios-wait-for-kibana-logging
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for kibana-logging"
             sleep 2;
             export COUNT=$(kubectl get pods -l k8s-app=kibana-logging -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w kibana-logging | wc -l)
             echo $COUNT
          done
      - name: mios-wait-for-alertstore-tablemanager
        image: mayadataio/maya-init:latest
        command:
        - sh
        - "-c"
        - |
          set -ex
          export COUNT=0
          echo $COUNT
          until [ $COUNT -ne 0 ]
          do
             echo "wating for alertstore-tablamanager"
             sleep 2;
             export COUNT=$(kubectl get pods -l name=alertstore-tablemanager -n variablename -o json  | jq -r '.items[] | select(.status.phase == "Running" and ([ .status.conditions[] | select(.type == "Ready" and .status == "True") ] | length ) >= 1 ) | .metadata.name' | grep -w alertstore-tablemanager | wc -l)
             echo $COUNT
          done
      containers:
        - name: maya-io
          args:
            - "--db-host"
            - "mysql-0.mysql.variablename.svc.cluster.local"
            - "--db-user"
            - "maya"
            - "--db-pass"
            - "maya"
            - "--db-name"
            - "maya"
          image: index.docker.io/mayadata/maya-io-server:1.0.0-RC1.ee
          imagePullPolicy: Always
          ports:
          - containerPort: 8080
          volumeMounts:
          - name: maya-store
            mountPath: /mayastore
          - name: maya-config
            mountPath: /var/lib/maya/etc/maya.properties
            subPath: maya.properties
      imagePullSecrets:
      - name: mayadatasecret
      volumes:
      - name: maya-store
        persistentVolumeClaim:
          claimName: mayastore
      - name: maya-config
        configMap:
          name: maya-config
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: maya-io
  name: maya-io
spec:
  type: NodePort
  ports:
  - name: mayaport
    port: 8080
    protocol: TCP
  selector:
    app: maya-io
---

